{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d7c7ea-9270-47de-b8ac-d33d91c71949",
   "metadata": {},
   "source": [
    "# Question 1: Running Ollama with Docker\n",
    "\n",
    "## Instructions: \n",
    "Let's run ollama with Docker. We will need to execute the same command as in the lectures:\n",
    "\n",
    "`docker run -it \\`\n",
    "    `--rm \\`\n",
    "    `-v ollama:/root/.ollama \\`\n",
    "    `-p 11434:11434 \\`\n",
    "    `--name ollama \\`\n",
    "    `ollama/ollama`\n",
    "\n",
    "What's the version of ollama client?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c71ade93-6bef-4b62-9d92-567376f6f2db",
   "metadata": {},
   "source": [
    "# After running ollama with docker, I enter the container created by docker using:\n",
    "docker exec -it ollama bash \n",
    "\n",
    "# To find the version of the ollama model client after entering the container, I ran:\n",
    "ollama -v\n",
    "\n",
    "# Ollama client version is: \n",
    "0.1.48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b166abfa-43ee-4a9f-9367-665c4b884fbf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9d2f69-485b-49ba-a1f3-820958fcf57e",
   "metadata": {},
   "source": [
    "# Question 2: Downloading an LLM\n",
    "We will donwload a smaller LLM - gemma:2b.\n",
    "\n",
    "## Instructions: \n",
    "Again let's enter the container and pull the model:\n",
    "\n",
    "`ollama pull gemma:2b`\n",
    "\n",
    "In docker, it saved the results into `/root/.ollama` \n",
    "\n",
    "We're interested in the metadata about this model. You can find it in `models/manifests/registry.ollama.ai/library`\n",
    "\n",
    "What's the content of the file related to gemma?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a44e4a2-b31a-4135-97e1-c92aec0a4d5b",
   "metadata": {},
   "source": [
    "# To enter the container {ollama}:\n",
    "I ran (docker exec -it ollama bash)\n",
    "\n",
    "# To pull the llm ollama model {gemma:2b} after entering the container:\n",
    "I ran (ollama pull gemma:2b)\n",
    "\n",
    "# After pulling the model in the container, I located the metadata about the model by the command:\n",
    "cd /root/.ollama/models/manifests/registry.ollama.ai/library\n",
    "\n",
    "## Getting to library directory, I ran:\n",
    "ls\n",
    "\n",
    "### Resulting to:\n",
    "gemma \n",
    "\n",
    "## Then I ran the below command to get the file inside gemma:\n",
    "ls gemma \n",
    "\n",
    "### Resulting to:\n",
    "2b\n",
    "\n",
    "## To get the metadata of 2b, I ran:\n",
    "cat 2b \n",
    "\n",
    "### Resulting to:\n",
    "{\"schemaVersion\":2,\"mediaType\":\"application/vnd.docker.distribution.manifest.v2+json\",\"config\":{\"mediaType\":\"application/vnd.docker.container.image.v1+json\",\"digest\":\"sha256:887433b89a901c156f7e6944442f3c9e57f3c55d6ed52042cbb7303aea994290\",\"size\":483},\"layers\":[{\"mediaType\":\"application/vnd.ollama.image.model\",\"digest\":\"sha256:c1864a5eb19305c40519da12cc543519e48a0697ecd30e15d5ac228644957d12\",\"size\":1678447520},{\"mediaType\":\"application/vnd.ollama.image.license\",\"digest\":\"sha256:097a36493f718248845233af1d3fefe7a303f864fae13bc31a3a9704229378ca\",\"size\":8433},{\"mediaType\":\"application/vnd.ollama.image.template\",\"digest\":\"sha256:109037bec39c0becc8221222ae23557559bc594290945a2c4221ab4f303b8871\",\"size\":136},{\"mediaType\":\"application/vnd.ollama.image.params\",\"digest\":\"sha256:22a838ceb7fb22755a3b0ae9b4eadde629d19be1f651f73efb8c6b4e2cd0eea0\",\"size\":84}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114d709-d8f9-48d9-8699-8cfe5da806eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381d713-b337-4a3b-b06c-bb1c39f989e4",
   "metadata": {},
   "source": [
    "# Question 3: Running the LLM\n",
    "\n",
    "Test the following prompt: \"10 * 10\". \n",
    "\n",
    "What's the answer?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a86756b-dfe1-4cf6-9a75-b6f3aa2990a2",
   "metadata": {},
   "source": [
    "# Inside the container, I ran;\n",
    "ollama run gemma:2b\n",
    "\n",
    "# After execution of the command, this will the llm model. I prompted the model using the below prompt:\n",
    "\"10 * 10\"\n",
    "\n",
    "## Resulting to:\n",
    "Sure, here is the answer to the question:\n",
    "\n",
    "10 * 10 = 100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cfea84-cb36-4171-88bb-1f4604813d9a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4878fb2c-8d84-4d68-98f8-649dfb268573",
   "metadata": {},
   "source": [
    "# Question 4: Donwloading the weights\n",
    "We don't want to pull the weights every time we run a docker container. Let's do it once and have them available every time we start a container.\n",
    "\n",
    "## Steps:\n",
    "- First, we will need to change how we run the container.\n",
    "- Instead of mapping the `/root/.ollama` folder to a named volume, let's map it to a local directory:\n",
    "    - First, run `mkdir ollama_files`\n",
    "    - `docker run -it \\\n",
    "    --rm \\\n",
    "    -v ./ollama_files:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama`\n",
    "    - Now pull the llm model inside the container by running: `docker exec -it ollama ollama pull gemma:2b`\n",
    "\n",
    "### Main Question\n",
    "\n",
    "**What's the size of the `ollama_files/models` folder?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "697a8ae3-bb7d-4408-ae9b-3ebf0c7da815",
   "metadata": {},
   "source": [
    "# After following the above steps, I ran the below command to get the model size which is:\n",
    "cd ollama_files\n",
    "\n",
    "# Then, I ran:\n",
    "du -sh models \n",
    "\n",
    "## Resulting to:\n",
    "1.6G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcfbd17-263b-4ae0-b79c-e0f18c5ec61f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd158d1-058b-44ee-9eb9-66bf678caa73",
   "metadata": {},
   "source": [
    "# Question 5: Adding the weights\n",
    "\n",
    "## Instructions:\n",
    "Let's now stop the container and add the weights to a new image\n",
    "- For that, let's create a Dockerfile:\n",
    "    - FROM ollama/ollama\n",
    "    - COPY ...\n",
    "\n",
    "### Main Question\n",
    "What do you put after COPY?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4824c48f-f9f7-4adf-b738-6f4d5294e3d1",
   "metadata": {},
   "source": [
    "# Use the official Ollama image as a base\n",
    "FROM ollama/ollama\n",
    "\n",
    "# Create the .ollama directory in the container\n",
    "RUN mkdir -p /root/.ollama\n",
    "\n",
    "# Copy the local ollama_files directory to the container\n",
    "COPY ollama_files /root/.ollama\n",
    "\n",
    "# Copy the entry script into the container\n",
    "COPY entrypoint.sh /usr/local/bin/entrypoint.sh\n",
    "\n",
    "# Set the entry script as executable\n",
    "RUN chmod +x /usr/local/bin/entrypoint.sh\n",
    "\n",
    "# Set the working directory\n",
    "WORKDIR /root/.ollama\n",
    "\n",
    "# Expose the port\n",
    "EXPOSE 11434\n",
    "\n",
    "# Command to run the entry script\n",
    "ENTRYPOINT [\"/usr/local/bin/entrypoint.sh\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4b6fe0-5a16-4797-b8aa-80bff638ddf6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a448415-4b6c-4034-84c0-3dab0cd70824",
   "metadata": {},
   "source": [
    "# Question: Serving it\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "- Let's build it, by running `docker build -t ollama-gemma2b`\n",
    "- Then run it, by running `docker run -it --rm -p 11434:11434 ollama-gemma2b`\n",
    "- Connect to it using OpenAI client\n",
    "    - Testing after the connection using the following prompt: `What's the formula for energy?`\n",
    "    - Make the result reproducible by setting `temperature` parameter to `0`\n",
    " \n",
    "## Main question\n",
    "How many completion tokens did you get in response?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ec50c-457c-4e17-a9de-08ce2a7b1502",
   "metadata": {},
   "source": [
    "### Workflow 1 "
   ]
  },
  {
   "cell_type": "raw",
   "id": "222c2287-7cc8-4870-8060-0573d4f7f304",
   "metadata": {},
   "source": [
    "# Dockerfile content\n",
    "Check the project directory for the docker file content code\n",
    "\n",
    "# Entrypoint script content\n",
    "Check the project directory for the entrypoint file content code \n",
    "\n",
    "# To make the entrypoint script executable, I ran:\n",
    "chmod +x entrypoint.sh\n",
    "\n",
    "# To build the Docker image:, I ran:\n",
    "docker build -t ollama-gemma2b .\n",
    "\n",
    "# To run the Docker container, I ran:\n",
    "docker run -it --rm -p 11434:11434 --name ollama ollama-gemma2b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551647d2-c2c5-4042-bef3-7871525c3bff",
   "metadata": {},
   "source": [
    "### Workflow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b35863bb-9968-4e67-aa03-8a9e8016d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to ollama container using OpenAI\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1935726d-7557-4ae3-8d05-d0ab038d5886",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What's the formula for energy?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f7751d-17fa-41f9-8105-845fe2d7c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    #...\n",
    "    model=\"gemma:2b\", \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a49f0f24-4306-4655-ad07-7930daf18fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "110486a7-c7db-404d-a912-03db8b03cffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's the formula for energy:\n",
      "\n",
      "**E = K + U**\n",
      "\n",
      "Where:\n",
      "\n",
      "* **E** is the energy in joules (J)\n",
      "* **K** is the kinetic energy in joules (J)\n",
      "* **U** is the potential energy in joules (J)\n",
      "\n",
      "**Kinetic energy (K)** is the energy an object possesses when it moves or is in motion. It is calculated as half the product of an object's mass (m) and its velocity (v) squared:\n",
      "\n",
      "**K = 1/2mv^2**\n",
      "\n",
      "**Potential energy (U)** is the energy an object possesses due to its position or configuration. It is calculated as the product of an object's mass, gravitational constant (g), and height or position above a reference point.\n",
      "\n",
      "**U = mgh**\n",
      "\n",
      "Where:\n",
      "\n",
      "* **m** is the mass in kilograms (kg)\n",
      "* **g** is the gravitational constant (9.8 m/s^2)\n",
      "* **h** is the height or position in meters (m)\n",
      "\n",
      "The formula shows that energy can be expressed as the sum of kinetic and potential energy. The kinetic energy is a measure of the object's ability to do work, while the potential energy is a measure of the object's ability to do work against a force.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69d4fcb5-bc05-458a-8cb7-728aede02878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the completion: 261\n"
     ]
    }
   ],
   "source": [
    "# importing tiktoken\n",
    "import tiktoken\n",
    "\n",
    "# Tokenize the completion text using tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # Make sure the encoding matches the model you are using\n",
    "tokens = tokenizer.encode(result)\n",
    "\n",
    "# Print the number of tokens\n",
    "print(\"Number of tokens in the completion:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19fc260-10e4-4627-9ccd-3fa14e1d42a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
